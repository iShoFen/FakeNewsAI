{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Fake News Study* / Etude Fake News\n",
    "\n",
    "*EN* // **FR**\n",
    "\n",
    "*Data files are stored in 'Data' repertory, the whole purpose of this work is to\n",
    "develop an AI that recognizes Fake News, train it and test it to bring it to the highest\n",
    "accuracy possible.*\n",
    "\n",
    "**Les données sont stockées dans le dossier 'Data', le but de ce travail est de développer une IA capable de reconnaître\n",
    "les fake news, l'entraîner, la tester et l'amener à la plus haute précision possible.**\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1639218142362
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Utils libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyarrow.feather as ft\n",
    "\n",
    "# AI libs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Creating a dataset to work with* / Création du dataset de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1639141056511
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fake = pd.read_csv('../Data/Fake.csv', delimiter=',')\n",
    "true = pd.read_csv('../Data/True.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We now need to merge these two dataframes to be able to use them as one whole entity.\n",
    "Nevertheless, we will need to differentiate real and fake news, so we will have to add\n",
    "an extra column as 'istrue'.*\n",
    "\n",
    "**Nous devons maintenant concaténer ces deux DataFrames pour pouvoir les utiliser en tant qu'une seule et même entité.\n",
    "Cependant, nous devons être capable de différencier les news fakes et réelles, nous allons donc rajouter une colonne\n",
    "de booléens 'istrue'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1639141057023
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fake['istrue'] = 0\n",
    "true['istrue'] = 1\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data = true.append(fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We now have a full dataframe with real and fake news, let's check the dimensions\n",
    "and fields we have.*\n",
    "\n",
    "**Nous avons maintenant un DataFrame complet avec les news réelles et fakes, vérifions dimensions et champs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639141057115
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Analyzing, cleaning and repairing data* / Analyse, nettoyage et réparation des données\n",
    "\n",
    "*Now that we have the dataset we needed, let's look for null or NaN values in it.*\n",
    "\n",
    "**Maintenant que nous avons le bon dataset, regardons s'il contient des valeurs NULL ou NaN.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639141057206
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can see from this execution that there is no null or NaN value in the dataset, so\n",
    "we won't have to repair any of the data, which is good news.\n",
    "<br>\n",
    "We will now delete the column we won't need during the study : date.*\n",
    "\n",
    "**Nous nous rendons compte suite à cette exécution qu'aucune valeur NULL ou NaN ne se trouve dans le dataset, nous n'aurons donc pas à réparer les données, ce qui est une bonne nouvelle.\n",
    "<br>\n",
    "Nous allons maintenant supprimer la colonne dont nous n'aurons pas besoin dans cette étude : date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639141057312
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns='date')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*However, we will need to split title contents into more simplified strings to avoid confusion\n",
    "during data processing.\n",
    "<br>\n",
    "To this purpose, we'll use the nltk and re libraries, which contain\n",
    "stopwords and string processing packages (called PortStemmer).\n",
    "But first, let's make an array that contains all titles.*\n",
    "\n",
    "**Cependant, nous allons avoir besoin de séparer les contenus des titres en des chaînes de caractères plus simplifiées pour éviter les confusions durant l'exploitation des données.\n",
    "<br>\n",
    "Pour cela, nous aurons besoin des bibliothèques nltk et re, qui contiennent des packages liés aux mots de liaison et aux traitements de chaînes de caractères (notamment le PortStemmer)**\n",
    "\n",
    "***Stopwords:***\n",
    "<br>\n",
    "*A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search\n",
    "engine has been programmed to ignore, both when indexing entries for searching\n",
    "and when retrieving them as the result of a search query.\n",
    "(https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)*\n",
    "<br>\n",
    "**Un mot de liaison est un mot couramment utilisé (comme \"et\", \"ou\", \"un\", etc) qu'un moteur de recherche a été programmé pour ignorer, que ce soit lors de l'indexation des entrées pour la recherche\n",
    "ou lors de leur récupération en tant que résultat d'une requête de recherche.\n",
    "(https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)**\n",
    "\n",
    "***PortStemmer:***\n",
    "<br>\n",
    "*A PortStemmer is an algorithm used for removing the commoner morphological and\n",
    "inflexional endings from words in English. For example: words such as “Likes”,\n",
    "”liked”, ”likely” and ”liking” will be reduced to “like” after stemming.\n",
    "(https://www.geeksforgeeks.org/python-stemming-words-with-nltk/)*\n",
    "<br>\n",
    "**Un PortStemmer est un algorithme utilisé pour supprimer les terminaisons morphologiques et inflexionnelles les plus courantes des mots.\n",
    "Par exemple, des mots tels que \"Likes\", \"liked\", \"likely\" et \"liking\" seront réduits à \"like\" après le stemming.\n",
    "(https://www.geeksforgeeks.org/python-stemming-words-with-nltk/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639144263954
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "titles = np.array(data['title'])\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "corpustitle = []\n",
    "\n",
    "for i in range (titles.shape[0]):\n",
    "    new = re.sub('[^a-zA-Z]', ' ', titles[i])\n",
    "    # Replaces any string matching the regex with spaces (anything other than a letter)\n",
    "\n",
    "    new = new.lower()\n",
    "\n",
    "    new = new.split()\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    new = [ps.stem(word) for word in new if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    new = ' '.join(new)\n",
    "\n",
    "    corpustitle.append(new)\n",
    "\n",
    "data['title'] = corpustitle\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639143840239
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "text = np.array(data['text'])\n",
    "\n",
    "corpustxt = []\n",
    "\n",
    "for i in range (text.shape[0]):\n",
    "    new = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "    # Replaces any string matching the regex with spaces (anything other than a letter)\n",
    "\n",
    "    new = new.lower()\n",
    "\n",
    "    new = new.split()\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    new = [ps.stem(word) for word in new if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    new = ' '.join(new)\n",
    "\n",
    "    corpustxt.append(new)\n",
    "\n",
    "data['text'] = corpustxt\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639144819716
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "feather = data.reset_index()\n",
    "feather.to_feather('../Data/datas.feather')\n",
    "\n",
    "data.to_csv('../Data/datas.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now that our titles are simplified, let's vectorize them using CountVectorizer\n",
    "(provided by sklearn)*\n",
    "<br>\n",
    "**Maintenant que nos titres sont simplifiés, vectorisons-les en utilisant CountVectorizer\n",
    "(fourni par sklearn)**\n",
    "\n",
    "***CountVectorizer:***\n",
    "<br>\n",
    "*CountVectorizer is a tool provided by the scikit-learn library. It is used to transform\n",
    "a given text into a vector on the basis of the frequency (count) of each word that occurs\n",
    "in the entire text.\n",
    "(https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/)*\n",
    "<br>\n",
    "**CountVectorizer est un outil fourni par la bibliothèque scikit-learn. Il est utilisé pour transformer\n",
    "un texte donné en un vecteur sur la base de la fréquence (comptage) de chaque mot qui apparaît\n",
    "dans le texte entier.\n",
    "(https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1639218190491
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "countv = CountVectorizer()\n",
    "data = pd.read_feather('../Data/datas.feather')\n",
    "\n",
    "# max_features: we are selecting the 5000 most used words in the whole dataset\n",
    "\n",
    "X1 = countv.fit_transform(data['title']).toarray().tolist()\n",
    "# Results are being encoded by the 'transform' part of the method\n",
    "\n",
    "y = data['istrue'].values\n",
    "\n",
    "data['title'] = X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "countv = CountVectorizer()\n",
    "\n",
    "# max_features: we are selecting the 5000 most used words in the whole dataset\n",
    "\n",
    "X2 = countv.fit_transform(data['text']).toarray().tolist()\n",
    "# Results are being encoded by the 'transform' part of the method\n",
    "\n",
    "data['text'] = X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1639149018105
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>istrue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              title  \\\n",
       "0      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                text       subject  istrue  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  politicsNews       1  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  politicsNews       1  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  politicsNews       1  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  politicsNews       1  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  politicsNews       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather = data.reset_index\n",
    "feather.to_feather('../Data/datas.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "countv = CountVectorizer()\n",
    "\n",
    "# max_features: we are selecting the 5000 most used words in the whole dataset\n",
    "\n",
    "X3 = countv.fit_transform(data['subject']).toarray().tolist()\n",
    "# Results are being encoded by the 'transform' part of the method\n",
    "\n",
    "data['subject'] = X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather = data.reset_index()\n",
    "feather.to_feather('../Data/datas.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather('../Data/datas.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>istrue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index                                              title  \\\n",
       "0        0      0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1        1      1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2        2      2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3        3      3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4        4      4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                       subject  istrue  \n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 0]       1  \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0]       1  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0]       1  \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 0, 0]       1  \n",
       "4  [0, 0, 0, 0, 0, 0, 1, 0, 0]       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.write_feather(data, '../Data/datacompressed.feather', compression='lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
